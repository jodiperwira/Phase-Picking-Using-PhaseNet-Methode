{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNPY7FYmX/HUMSXQauM/6mN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jodiperwira/Phase-Picking-Using-PhaseNet-Methode/blob/main/PhaseNet%20on%20Dataset%20Indo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRjZi3BJ2CTp"
      },
      "outputs": [],
      "source": [
        "!pip install seisbench\n",
        "!pip install obspy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8AnRZ6R2mlU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTCzDGHA3LRk"
      },
      "outputs": [],
      "source": [
        "# @title Library\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import obspy\n",
        "from obspy.core import UTCDateTime\n",
        "import joblib\n",
        "from keras import models\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "import seisbench\n",
        "import seisbench.generate as sbg\n",
        "import seisbench.models as sbm\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgvB8MZ_3a6Z"
      },
      "outputs": [],
      "source": [
        "list_values = np.load('/content/drive/MyDrive/PhaseNet/Hasil/Interpolate/Data2/Hasil3(fix)_Jatim_100Hz.npy')\n",
        "list_pphase = np.load('/content/drive/MyDrive/PhaseNet/Hasil/Interpolate/Data2/plabel(fix)_100Hz.npy')\n",
        "list_sphase = np.load('/content/drive/MyDrive/PhaseNet/Hasil/Interpolate/Data2/slabel(fix)_100Hz.npy')\n",
        "error_indices = np.load('/content/drive/MyDrive/PhaseNet/Hasil/Interpolate/Data2/error_indices(fix)_.npy')\n",
        "print(list_values.shape)\n",
        "print(list_pphase.shape)\n",
        "print(list_sphase.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSm4nhKd4LKc"
      },
      "outputs": [],
      "source": [
        "data = np.delete(list_values, error_indices, axis=0)\n",
        "\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.count_nonzero(np.isnan(data))"
      ],
      "metadata": {
        "id": "ph4CDjU_THms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi list untuk menyimpan \"slices\" unik\n",
        "unique_slices = [data[0]]\n",
        "deleted_indices = []\n",
        "\n",
        "# Iterasi melalui array dan tambahkan \"slice\" yang unik ke list\n",
        "for i in range(1, data.shape[0]):\n",
        "    if not np.array_equal(data[i], data[i-1]):\n",
        "        unique_slices.append(data[i])\n",
        "    else:\n",
        "        deleted_indices.append(i-1)\n",
        "\n",
        "# Ubah list kembali ke array 3D\n",
        "cleaned_data = np.array(unique_slices)"
      ],
      "metadata": {
        "id": "kENLVuvNeq7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJXp5QHz6s_n"
      },
      "outputs": [],
      "source": [
        "cleaned_plabel = np.delete(list_pphase, deleted_indices, axis=0)\n",
        "cleaned_slabel = np.delete(list_sphase, deleted_indices, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq6kKsqY60Bv"
      },
      "outputs": [],
      "source": [
        "print(\"data: \", cleaned_data.shape)\n",
        "print(\"p label: \", cleaned_plabel.shape)\n",
        "print(\"s label: \", cleaned_slabel.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AOkfreK63Vp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "# Combine E, N, and Z in one subplot\n",
        "plt.subplot(211)\n",
        "plt.plot(data[892][:,0], label=\"E\")\n",
        "plt.plot(data[892][:,1], label=\"N\")\n",
        "plt.plot(data[892][:,2], label=\"Z\")\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Plot P-confirmed and S-confirmed\n",
        "plt.subplot(212)\n",
        "plt.title(\"Grafik Analisa Pengujian Data Asli terhadap Hasil Prediksi\", fontsize=10)\n",
        "plt.xlabel(\"Time\", fontsize=10)\n",
        "plt.ylabel(\"Probabilitas\", fontsize=10)\n",
        "plt.plot(list_pphase[892], label=\"P-confirmed\")\n",
        "plt.plot(list_sphase[892], label=\"S-confirmed\")\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NB0VQp887eEV"
      },
      "outputs": [],
      "source": [
        "arr_reshaped = np.moveaxis(cleaned_data, -1, -2)\n",
        "\n",
        "arr_reshaped.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTzyJSdQ7X28"
      },
      "outputs": [],
      "source": [
        "poin_p = np.where(cleaned_plabel == 1)\n",
        "max_data = np.zeros([arr_reshaped.shape[0]])\n",
        "min_data = np.zeros([arr_reshaped.shape[0]])\n",
        "\n",
        "for i, v in zip(poin_p[0], poin_p[1]):\n",
        "    max_data[i] = arr_reshaped[i, :, v:].max(0).max(0)\n",
        "    min_data[i] = arr_reshaped[i, :, v:].min(0).min(0)\n",
        "\n",
        "# Ganti seluruh nilai yang lebih besar dari max_data pada setiap indeks dan channel data dengan 0\n",
        "for i in range(arr_reshaped.shape[0]):\n",
        "    for j in range(arr_reshaped.shape[1]):\n",
        "        for z in range(arr_reshaped.shape[2]):\n",
        "            if arr_reshaped[i, j, z] > max_data[i]:\n",
        "                arr_reshaped[i, j, z] = 0\n",
        "            if arr_reshaped[i, j, z] < 0:\n",
        "                if arr_reshaped[i, j, z] < min_data[i]:\n",
        "                    arr_reshaped[i, j, z] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BotOe-J9XUJ"
      },
      "outputs": [],
      "source": [
        "print(\"data: \", arr_reshaped.shape)\n",
        "print(\"p label: \", cleaned_plabel.shape)\n",
        "print(\"s label: \", cleaned_slabel.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxBWs3qv_9nN"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "# Combine E, N, and Z in one subplot\n",
        "plt.subplot(211)\n",
        "plt.plot(arr_reshaped[892][0,:], label=\"E\")\n",
        "plt.plot(arr_reshaped[892][1,:], label=\"N\")\n",
        "plt.plot(arr_reshaped[892][2,:], label=\"Z\")\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Plot P-confirmed and S-confirmed\n",
        "plt.subplot(212)\n",
        "plt.plot(cleaned_plabel[892], label=\"P-confirmed\")\n",
        "plt.plot(cleaned_slabel[892], label=\"S-confirmed\")\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CQ3nswxAwI3"
      },
      "outputs": [],
      "source": [
        "combined_labels = np.stack((cleaned_plabel, cleaned_slabel))\n",
        "combined_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6dOJQmU_-AV"
      },
      "outputs": [],
      "source": [
        "def gaussian(x, mu=0, sigma=10):\n",
        "    return np.exp(-(x - mu) ** 2 / (2 * sigma ** 2))\n",
        "\n",
        "sigma = 10\n",
        "data_sum = combined_labels[0].shape[0]\n",
        "\n",
        "prob_labels = np.zeros_like(combined_labels, dtype=np.float32)\n",
        "\n",
        "# Untuk label P (indeks 0)\n",
        "for i in range(data_sum):\n",
        "    for j in range(3000):  # Change 3001 to 3000 to stay within array bounds\n",
        "        if combined_labels[0, i, j] == 1:\n",
        "            for offset in range(-3*sigma, 3*sigma):  # 3*sigma sebagai jangkauan untuk mendapatkan sebagian besar efek Gaussian\n",
        "                if 0 <= j + offset < 3000:  # Change 3001 to 3000 to stay within array bounds\n",
        "                    prob_labels[0, i, j+offset] += gaussian(offset)\n",
        "\n",
        "# Sama untuk label S (indeks 1)\n",
        "for i in range(data_sum):\n",
        "    for j in range(3000):  # 3000\n",
        "        if combined_labels[1, i, j] == 1:\n",
        "            for offset in range(-3*sigma, 3*sigma):\n",
        "                if 0 <= j + offset < 3000:  # 3000\n",
        "                    prob_labels[1, i, j+offset] += gaussian(offset)\n",
        "\n",
        "# Normalisasi jika diperlukan\n",
        "prob_labels = np.clip(prob_labels, 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpnQtZbfLpcC"
      },
      "outputs": [],
      "source": [
        "prob_noise = np.ones_like(prob_labels[0], dtype=np.float32)\n",
        "\n",
        "for i in range(data_sum):\n",
        "    for j in range(3000):  # Assuming the range is up to 3000 based on your code\n",
        "        prob_noise[i, j] = 1 - prob_labels[0, i, j] - prob_labels[1, i, j]\n",
        "\n",
        "prob_noise = np.clip(prob_noise, 0, 1)\n",
        "\n",
        "plt.plot(prob_noise[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zweWS9apMjTx"
      },
      "outputs": [],
      "source": [
        "prob_noise_expanded = prob_noise[np.newaxis, :, :]\n",
        "final_labels = np.concatenate((prob_noise_expanded, prob_labels), axis=0)\n",
        "\n",
        "final_labels[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSfcSFzsA7QJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "\n",
        "# Combine E, N, and Z in one subplot\n",
        "plt.subplot(211)\n",
        "plt.plot(arr_reshaped[892][0,:], label=\"E\")\n",
        "plt.plot(arr_reshaped[892][1,:], label=\"N\")\n",
        "plt.plot(arr_reshaped[892][2,:], label=\"Z\")\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Plot P-confirmed and S-confirmed\n",
        "plt.subplot(212)\n",
        "plt.title(\"Label\", fontsize=10)\n",
        "plt.xlabel(\"Time\", fontsize=10)\n",
        "plt.ylabel(\"Amplitude\", fontsize=10)\n",
        "# plt.plot(final_labels[0][892], label=\"N-confirmed\")\n",
        "plt.plot(final_labels[1][892], label=\"P-confirmed\")\n",
        "plt.plot(final_labels[2][892], label=\"S-confirmed\")\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLyU8J1dBSNt"
      },
      "outputs": [],
      "source": [
        "np.save('/content/drive/MyDrive/Jodi/Data2/fian_clean_labels_100Hz.npy', prob_labels)\n",
        "np.save('/content/drive/MyDrive/Jodi/Data2/fian_clean_data_100Hz.npy', arr_reshaped)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob_labels = np.load('/content/drive/MyDrive/PhaseNet/Hasil/Model/clean_labels_100Hz_sigma10.npy')\n",
        "arr_reshaped = np.load('/content/drive/MyDrive/PhaseNet/Hasil/Model/clean_data_100Hz_sigma10.npy')"
      ],
      "metadata": {
        "id": "TMjEkC5iFsBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PEd0c1NBuFR"
      },
      "outputs": [],
      "source": [
        "def standard_deviation_normalization(data):\n",
        "    mean = np.mean(data)\n",
        "    std_dev = np.std(data)\n",
        "    normalized_data = (data - mean) / std_dev\n",
        "    return normalized_data\n",
        "\n",
        "std_deviation_data = standard_deviation_normalization(arr_reshaped)\n",
        "\n",
        "std_deviation_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fARAnxFrQTCz"
      },
      "outputs": [],
      "source": [
        "def robust_scaling(data):\n",
        "    median = np.median(data)\n",
        "    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
        "\n",
        "    robust_scaled_data = (data - median) / iqr\n",
        "\n",
        "    return robust_scaled_data\n",
        "\n",
        "robust_data = robust_scaling(arr_reshaped)\n",
        "\n",
        "robust_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_j8_YbVQijJ"
      },
      "outputs": [],
      "source": [
        "def max_abs_scaling(data):\n",
        "    max_abs = np.max(np.abs(data))\n",
        "\n",
        "    if max_abs == 0:\n",
        "        # Avoid division by zero\n",
        "        return data\n",
        "    else:\n",
        "        max_abs_scaled_data = data / max_abs\n",
        "        return max_abs_scaled_data\n",
        "\n",
        "max_abs_data = max_abs_scaling(arr_reshaped)\n",
        "\n",
        "max_abs_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "# Combine E, N, and Z in one subplot\n",
        "plt.subplot(111)\n",
        "plt.plot(std_deviation_data[0][0,:], label=\"E\")\n",
        "plt.plot(std_deviation_data[0][1,:], label=\"N\")\n",
        "plt.plot(std_deviation_data[0][2,:], label=\"Z\")\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TWtMf2ZO7fex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "# Combine E, N, and Z in one subplot\n",
        "plt.subplot(111)\n",
        "plt.plot(max_abs_data[0][0,:], label=\"E\")\n",
        "plt.plot(max_abs_data[0][1,:], label=\"N\")\n",
        "plt.plot(max_abs_data[0][2,:], label=\"Z\")\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5gMjKioU8x08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "# Combine E, N, and Z in one subplot\n",
        "plt.subplot(111)\n",
        "plt.plot(robust_data[0][0,:], label=\"E\")\n",
        "plt.plot(robust_data[0][1,:], label=\"N\")\n",
        "plt.plot(robust_data[0][2,:], label=\"Z\")\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0syRps7T84tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikDJy0XjCLMM"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]  # mengembalikan jumlah total sampel\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx]\n",
        "        y = self.labels[:, idx]  # ambil data dari dua label P dan S\n",
        "\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "674Oj0-uLBXv"
      },
      "outputs": [],
      "source": [
        "total = std_deviation_data.shape[0]\n",
        "\n",
        "train = int(0.8 * total)\n",
        "val = int(0.1 * total)\n",
        "test = int(total - train - val)\n",
        "\n",
        "x_train = std_deviation_data[:train]\n",
        "y_train = final_labels[:,:train]\n",
        "\n",
        "x_val = std_deviation_data[train:train + val]\n",
        "y_val = final_labels[:,train:train + val]\n",
        "\n",
        "x_test = std_deviation_data[train + val:]\n",
        "y_test = final_labels[:,train + val:]\n",
        "\n",
        "train_size = x_train.shape[0]\n",
        "val_size = x_val.shape[0]\n",
        "test_size = x_test.shape[0]\n",
        "\n",
        "print(\"Jumlah data set train: \", train_size)\n",
        "print(\"Jumlah data set val: \", val_size)\n",
        "print(\"Jumlah data set test: \", test_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xvg30-ecCcUR"
      },
      "outputs": [],
      "source": [
        "train_generator = CustomDataset(x_train, y_train)\n",
        "dev_generator = CustomDataset(x_val, y_val)\n",
        "test_generator = CustomDataset(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uzO97N6GNiK"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_generator, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(dev_generator, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5f59_mML3_O"
      },
      "outputs": [],
      "source": [
        "model = sbm.PhaseNet.from_pretrained(\"original\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_xMz-LG2UW9"
      },
      "outputs": [],
      "source": [
        "model = sbm.PhaseNet(phases=\"PSN\",norm=\"peak\")\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bYwprMfQ6Bb"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "epochs = 20\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def loss_fn(y_pred, y_true, eps=1e-5):\n",
        "    # vector cross entropy loss\n",
        "    h = y_true * torch.log(y_pred + eps)\n",
        "    h = h.mean(-1).sum(-1)  # Mean along sample dimension and sum along pick dimension\n",
        "    h = h.mean()  # Mean over batch axis\n",
        "    return -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRCsCkGcFYm4"
      },
      "outputs": [],
      "source": [
        "def f1(y_true, y_pred, threshold):\n",
        "    def recall(y_true, y_pred, threshold):\n",
        "        true_positives = torch.sum((y_true * y_pred).float())\n",
        "        possible_positives = torch.sum(y_true.float() > threshold)\n",
        "        recall = true_positives / (possible_positives + torch.finfo(torch.float32).eps)\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred, threshold):\n",
        "        true_positives = torch.sum((y_true * y_pred).float())\n",
        "        predicted_positives = torch.sum(y_pred.float() > threshold)\n",
        "        precision = true_positives / (predicted_positives + torch.finfo(torch.float32).eps)\n",
        "        return precision\n",
        "\n",
        "    precision_value = precision(y_true, y_pred, threshold)\n",
        "    recall_value = recall(y_true, y_pred, threshold)\n",
        "    f1_value = 2 * ((precision_value * recall_value) / (precision_value + recall_value + torch.finfo(torch.float32).eps))\n",
        "\n",
        "    return f1_value.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def mae(y_true, y_pred, tolerance):\n",
        "    mae_total = 0\n",
        "    count = 0  # To keep track of instances where error > tolerance\n",
        "\n",
        "    for i in range(y_true.shape[0]):\n",
        "        if 1 in y_true[i]:\n",
        "            y_true_idx = torch.argmax(y_true[i], dim=0).float()\n",
        "            y_pred_idx = torch.argmax(y_pred[i], dim=0).float()\n",
        "            error = torch.abs(y_true_idx - y_pred_idx)\n",
        "\n",
        "            # Only include errors that are above the tolerance level\n",
        "            if error >= tolerance:\n",
        "                mae_total += error\n",
        "                count += 1  # Count this instance for averaging\n",
        "\n",
        "    # Calculate average MAE for instances where error > tolerance\n",
        "    if count > 0:  # Ensure we're not dividing by zero\n",
        "        mae_avg = mae_total / count\n",
        "        return mae_avg.item() / 100  # Assuming you want to keep the division by 100\n",
        "    else:\n",
        "        mae_avg = 0\n",
        "        return mae_avg\n"
      ],
      "metadata": {
        "id": "a6kIvXW-vu9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wEAjTug_e2L"
      },
      "outputs": [],
      "source": [
        "# Define the threshold value\n",
        "threshold = 0.5\n",
        "\n",
        "def train_loop(dataloader):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch_id, (data_batch, label_batch) in enumerate(dataloader):\n",
        "        data_batch, label_batch = data_batch.float().to(model.device), label_batch.float().to(model.device)\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(data_batch.to(model.device))\n",
        "        loss = loss_fn(pred, label_batch.to(model.device))\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred_binary = pred.detach().float()\n",
        "        f1_p = f1(label_batch[:, 1, :], pred_binary[:, 1, :], threshold)\n",
        "        f1_s = f1(label_batch[:, 2, :], pred_binary[:, 2, :], threshold)\n",
        "\n",
        "        mae_p = mae(label_batch[:, 1, :], pred_binary[:, 1, :], threshold)\n",
        "        mae_s = mae(label_batch[:, 2, :], pred_binary[:, 2, :], threshold)\n",
        "\n",
        "        if batch_id % 5 == 0:\n",
        "            loss, current = loss.item(), batch_id * data_batch.shape[0]\n",
        "            print(f\"loss: {loss:>7f}, F1 Score P: {f1_p:>7f}, F1 Score S: {f1_s:>7f}, MAE P: {mae_p:>7f}, MAE S: {mae_s:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    return loss, f1_p, f1_s, mae_p, mae_s\n",
        "\n",
        "def test_loop(dataloader):\n",
        "    num_batches = len(dataloader)\n",
        "    val_test_loss = 0\n",
        "    val_f1_p = 0\n",
        "    val_f1_s = 0\n",
        "    val_mae_p = 0\n",
        "    val_mae_s = 0\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (data_batch, label_batch) in dataloader:\n",
        "            data_batch, label_batch = data_batch.float().to(model.device), label_batch.float().to(model.device)\n",
        "            pred = model(data_batch.to(model.device))\n",
        "            val_test_loss += loss_fn(pred, label_batch.to(model.device)).item()\n",
        "            pred_binary = pred.detach().float()\n",
        "            val_f1_p += f1(label_batch[:, 1, :], pred_binary[:, 1, :], threshold)\n",
        "            val_f1_s += f1(label_batch[:, 2, :], pred_binary[:, 2, :], threshold)\n",
        "            val_mae_p += mae(label_batch[:, 1, :], pred_binary[:, 1, :], threshold)\n",
        "            val_mae_s += mae(label_batch[:, 2, :], pred_binary[:, 2, :], threshold)\n",
        "\n",
        "    model.train()  # Set the model back to training mode\n",
        "\n",
        "    val_test_loss /= num_batches\n",
        "    val_f1_p /= num_batches\n",
        "    val_f1_s /= num_batches\n",
        "    val_mae_p /= num_batches\n",
        "    val_mae_s /= num_batches\n",
        "    print(f\"Val avg loss: {val_test_loss:>8f}, Val avg F1-P: {val_f1_p:>8f}, Val avg F1-S: {val_f1_s:>8f}, Val avg MAE-P: {val_mae_p:>8f}, Val avg MAE-S: {val_mae_s:>8f} \\n\")\n",
        "\n",
        "    return val_test_loss, val_f1_p, val_f1_s, val_mae_p, val_mae_s\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_history = []\n",
        "f1_scorep_history = []\n",
        "f1_scores_history = []\n",
        "mae_p_history = []\n",
        "mae_s_history = []\n",
        "\n",
        "# # Set a patience threshold for early stopping\n",
        "# patience = 12  # Adjust this value as needed\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "\n",
        "    # Train loop\n",
        "    loss, f1_p, f1_s, mae_p, mae_s = train_loop(train_loader)\n",
        "\n",
        "    # Test loop\n",
        "    val_test_loss, val_f1_p, val_f1_s, val_mae_p, val_mae_s = test_loop(val_loader)\n",
        "\n",
        "    # Append results to history lists\n",
        "    loss_history.append(val_test_loss)\n",
        "    f1_scorep_history.append(val_f1_p)\n",
        "    f1_scores_history.append(val_f1_s)\n",
        "    mae_p_history.append(val_mae_p)\n",
        "    mae_s_history.append(val_mae_s)\n",
        "\n",
        "    # # Early stopping logic\n",
        "    # if t >= patience:  # Ensure enough epochs for initial evaluation\n",
        "    #     if val_test_loss >= min(loss_history[t-patience:]):  # Compare with minimum loss in patience window\n",
        "    #         print(f\"Early stopping at epoch {t + 1}\")\n",
        "    #         break\n"
      ],
      "metadata": {
        "id": "P4aJC6aoBNd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Daftar variabel\n",
        "variables = ['loss_history', 'f1_scorep_history', 'f1_scores_history', 'mae_p_history', 'mae_s_history']\n",
        "\n",
        "# Daftar nilai variabel\n",
        "values = [loss_history, f1_scorep_history, f1_scores_history, mae_p_history, mae_s_history]\n",
        "\n",
        "# Nama file CSV\n",
        "filename = \"training_metrics.csv\"\n",
        "\n",
        "# Menulis ke dalam file CSV\n",
        "with open(filename, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Menulis header\n",
        "    writer.writerow(variables)\n",
        "\n",
        "    # Menulis nilai\n",
        "    for i in range(len(values[0])):\n",
        "        row = [values[j][i] for j in range(len(values))]\n",
        "        writer.writerow(row)\n"
      ],
      "metadata": {
        "id": "pYh30T5ZsOX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "truy8guMtYYr"
      },
      "outputs": [],
      "source": [
        "sample = test_generator[np.random.randint(len(test_generator))]\n",
        "# Move the tensor to CPU and ensure it is in float format\n",
        "pred = model(torch.tensor([sample[0]]).float().to(model.device))\n",
        "y_pred = pred[:,1:].cpu().detach().numpy()\n",
        "\n",
        "# Rest of your plotting code remains the same\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "axs = fig.subplots(3, 1, sharex=True, gridspec_kw={\"hspace\": 0, \"height_ratios\": [3, 1, 1]})\n",
        "axs[0].plot(sample[0].T)\n",
        "axs[1].plot(sample[1].T)\n",
        "# Move the prediction back to CPU if it was on GPU for some reason, and detach it from the current graph\n",
        "axs[2].plot(y_pred[0].T)\n",
        "\n",
        "plt.show()  # Don't forget to show the plot if this is the end of your plotting code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6o4EmK6KVczH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Misalkan epochs adalah jumlah epochs yang Anda gunakan\n",
        "epochs = len(loss_history)  # Or the actual number of epochs\n",
        "\n",
        "# Assuming loss and val_test_loss are already NumPy arrays\n",
        "loss = loss  # No detach needed\n",
        "val_test_loss = val_test_loss  # No detach needed\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, epochs + 1), loss_history, label='Loss')\n",
        "plt.plot(range(1, epochs + 1), val_test_loss, label='Validation Loss')\n",
        "plt.title('Loss selama Training dan Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# # Membuat plot untuk F1 Score\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.plot(range(1, epochs + 1), f1_scorep_history, label='F1 Score Positive')\n",
        "# plt.plot(range(1, epochs + 1), f1_scores_history, label='F1 Score Negative')\n",
        "# plt.title('F1 Score selama Training dan Validation')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('F1 Score')\n",
        "# plt.legend()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSqiy6HxnCFG"
      },
      "outputs": [],
      "source": [
        "x_test = test_generator[:][0]\n",
        "y_test = test_generator[:][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4OBqfcek-gP"
      },
      "outputs": [],
      "source": [
        "pred = model(torch.tensor(x_test).float().to(model.device))\n",
        "pred = pred.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YKf4kRThIIA"
      },
      "outputs": [],
      "source": [
        "# Assuming y_tes and pred are tensors of shape (batch_size, time_steps, num_classes)\n",
        "\n",
        "# Menghitung MAE\n",
        "mae_p = (y_test[1].argmax(1) - pred[:,1,:].argmax(1)) / 100\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "# Ensure the mean is calculated over the correct dimensions and converted properly\n",
        "plt.text(-0.85, 125, r'$mae=$' + f'{round(abs(y_test[1].argmax(1) - pred[:,1,:].argmax(1)).mean()/100, 5)} s')\n",
        "plt.xlim(-2, 2)\n",
        "plt.title('P-Wave')\n",
        "plt.xlabel('MAE Values (seconds)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(np.linspace(-2, 2, 9))\n",
        "\n",
        "# Converting the tensor to a numpy array for histogram plotting\n",
        "plt.hist(mae_p.flatten(), 24, alpha=0.5, edgecolor='black')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWHq3t0ese6k"
      },
      "outputs": [],
      "source": [
        "idx = np.where(y_test[2] == 1)[0]\n",
        "\n",
        "mask = np.isin(np.arange(y_test.shape[1]), idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_eQ1t-Whdzy"
      },
      "outputs": [],
      "source": [
        "y_test_s = y_test[:, mask, :]\n",
        "y_test_s.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1yBXaXt_VNM"
      },
      "outputs": [],
      "source": [
        "mask = np.isin(np.arange(pred.shape[0]), idx)\n",
        "\n",
        "# Terapkan mask ke pred\n",
        "pred_s = pred[mask, :, :]\n",
        "pred_s.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming y_tes and pred are tensors of shape (batch_size, time_steps, num_classes)\n",
        "\n",
        "# Menghitung MAE\n",
        "mae_s = (y_test_s[2].argmax(1) - pred_s[:,2,:].argmax(1)) / 100\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "# Ensure the mean is calculated over the correct dimensions and converted properly\n",
        "plt.text(-8.85, 125, r'$mae=$' + f'{round(abs(y_test_s[2].argmax(1) - pred_s[:,2,:].argmax(1)).mean()/100, 5)} s')\n",
        "plt.xlim(-10, 10)\n",
        "plt.title('S-Wave')\n",
        "plt.xlabel('MAE Values (seconds)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(np.linspace(-10, 10, 9))\n",
        "\n",
        "# Converting the tensor to a numpy array for histogram plotting\n",
        "plt.hist(mae_s.flatten(), 24, alpha=0.5, edgecolor='black')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dxvf9MoYV4-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXCWNyFsV7UE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}